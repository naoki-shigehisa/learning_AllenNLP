{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b917581c-0804-4687-baee-c332b0fea689",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'GradientDescentTrainer' from 'allennlp.training.trainer' (/usr/local/lib/python3.7/site-packages/allennlp/training/trainer.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_45/2314680836.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_field_embedders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBasicTextFieldEmbedder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGradientDescentTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCategoricalAccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'GradientDescentTrainer' from 'allennlp.training.trainer' (/usr/local/lib/python3.7/site-packages/allennlp/training/trainer.py)"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "from typing import Dict, Iterable, List, Tuple\n",
    "\n",
    "import allennlp\n",
    "import torch\n",
    "from allennlp.data import (\n",
    "    DataLoader,\n",
    "    DatasetReader,\n",
    "    Instance,\n",
    "    Vocabulary,\n",
    "    TextFieldTensors,\n",
    ")\n",
    "from allennlp.data.data_loaders import SimpleDataLoader\n",
    "from allennlp.data.fields import LabelField, TextField\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token, Tokenizer, WhitespaceTokenizer\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules import TextFieldEmbedder, Seq2VecEncoder\n",
    "from allennlp.modules.seq2vec_encoders import BagOfEmbeddingsEncoder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.nn import util\n",
    "from allennlp.training.trainer import GradientDescentTrainer, Trainer\n",
    "from allennlp.training.optimizers import AdamOptimizer\n",
    "from allennlp.training.metrics import CategoricalAccuracy\n",
    "\n",
    "\n",
    "class ClassificationTsvReader(DatasetReader):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: Tokenizer = None,\n",
    "        token_indexers: Dict[str, TokenIndexer] = None,\n",
    "        max_tokens: int = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.tokenizer = tokenizer or WhitespaceTokenizer()\n",
    "        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "        self.max_tokens = max_tokens\n",
    "\n",
    "    def _read(self, file_path: str) -> Iterable[Instance]:\n",
    "        with open(file_path, \"r\") as lines:\n",
    "            for line in lines:\n",
    "                text, sentiment = line.strip().split(\"\\t\")\n",
    "                tokens = self.tokenizer.tokenize(text)\n",
    "                if self.max_tokens:\n",
    "                    tokens = tokens[: self.max_tokens]\n",
    "                text_field = TextField(tokens, self.token_indexers)\n",
    "                label_field = LabelField(sentiment)\n",
    "                yield Instance({\"text\": text_field, \"label\": label_field})\n",
    "\n",
    "\n",
    "class SimpleClassifier(Model):\n",
    "    def __init__(\n",
    "        self, vocab: Vocabulary, embedder: TextFieldEmbedder, encoder: Seq2VecEncoder\n",
    "    ):\n",
    "        super().__init__(vocab)\n",
    "        self.embedder = embedder\n",
    "        self.encoder = encoder\n",
    "        num_labels = vocab.get_vocab_size(\"labels\")\n",
    "        self.classifier = torch.nn.Linear(encoder.get_output_dim(), num_labels)\n",
    "\n",
    "    def forward(\n",
    "        self, text: TextFieldTensors, label: torch.Tensor\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        print(\"In model.forward(); printing here just because binder is so slow\")\n",
    "        # Shape: (batch_size, num_tokens, embedding_dim)\n",
    "        embedded_text = self.embedder(text)\n",
    "        # Shape: (batch_size, num_tokens)\n",
    "        mask = util.get_text_field_mask(text)\n",
    "        # Shape: (batch_size, encoding_dim)\n",
    "        encoded_text = self.encoder(embedded_text, mask)\n",
    "        # Shape: (batch_size, num_labels)\n",
    "        logits = self.classifier(encoded_text)\n",
    "        # Shape: (batch_size, num_labels)\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        # Shape: (1,)\n",
    "        loss = torch.nn.functional.cross_entropy(logits, label)\n",
    "        return {\"loss\": loss, \"probs\": probs}\n",
    "\n",
    "\n",
    "def build_dataset_reader() -> DatasetReader:\n",
    "    return ClassificationTsvReader()\n",
    "\n",
    "\n",
    "def read_data(reader: DatasetReader) -> Tuple[List[Instance], List[Instance]]:\n",
    "    print(\"Reading data\")\n",
    "    training_data = list(reader.read(\"quick_start/data/movie_review/train.tsv\"))\n",
    "    validation_data = list(reader.read(\"quick_start/data/movie_review/dev.tsv\"))\n",
    "    return training_data, validation_data\n",
    "\n",
    "\n",
    "def build_vocab(instances: Iterable[Instance]) -> Vocabulary:\n",
    "    print(\"Building the vocabulary\")\n",
    "    return Vocabulary.from_instances(instances)\n",
    "\n",
    "\n",
    "def build_model(vocab: Vocabulary) -> Model:\n",
    "    print(\"Building the model\")\n",
    "    vocab_size = vocab.get_vocab_size(\"tokens\")\n",
    "    embedder = BasicTextFieldEmbedder(\n",
    "        {\"tokens\": Embedding(embedding_dim=10, num_embeddings=vocab_size)}\n",
    "    )\n",
    "    encoder = BagOfEmbeddingsEncoder(embedding_dim=10)\n",
    "    return SimpleClassifier(vocab, embedder, encoder)\n",
    "\n",
    "\n",
    "def run_training_loop():\n",
    "    dataset_reader = build_dataset_reader()\n",
    "\n",
    "    train_data, dev_data = read_data(dataset_reader)\n",
    "\n",
    "    vocab = build_vocab(train_data + dev_data)\n",
    "    model = build_model(vocab)\n",
    "\n",
    "    train_loader, dev_loader = build_data_loaders(train_data, dev_data)\n",
    "    train_loader.index_with(vocab)\n",
    "    dev_loader.index_with(vocab)\n",
    "\n",
    "    # You obviously won't want to create a temporary file for your training\n",
    "    # results, but for execution in binder for this guide, we need to do this.\n",
    "    with tempfile.TemporaryDirectory() as serialization_dir:\n",
    "        trainer = build_trainer(model, serialization_dir, train_loader, dev_loader)\n",
    "        print(\"Starting training\")\n",
    "        trainer.train()\n",
    "        print(\"Finished training\")\n",
    "\n",
    "    return model, dataset_reader\n",
    "\n",
    "\n",
    "def build_data_loaders(\n",
    "    train_data: List[Instance],\n",
    "    dev_data: List[Instance],\n",
    ") -> Tuple[DataLoader, DataLoader]:\n",
    "    train_loader = SimpleDataLoader(train_data, 8, shuffle=True)\n",
    "    dev_loader = SimpleDataLoader(dev_data, 8, shuffle=False)\n",
    "    return train_loader, dev_loader\n",
    "\n",
    "\n",
    "def build_trainer(\n",
    "    model: Model,\n",
    "    serialization_dir: str,\n",
    "    train_loader: DataLoader,\n",
    "    dev_loader: DataLoader,\n",
    ") -> Trainer:\n",
    "    parameters = [(n, p) for n, p in model.named_parameters() if p.requires_grad]\n",
    "    optimizer = AdamOptimizer(parameters)  # type: ignore\n",
    "    trainer = GradientDescentTrainer(\n",
    "        model=model,\n",
    "        serialization_dir=serialization_dir,\n",
    "        data_loader=train_loader,\n",
    "        validation_data_loader=dev_loader,\n",
    "        num_epochs=5,\n",
    "        optimizer=optimizer,\n",
    "        cuda_device=-1,\n",
    "    )\n",
    "    return trainer\n",
    "\n",
    "\n",
    "run_training_loop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
